{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Case Generation Pipeline\n",
    "\n",
    "This notebook demonstrates a pipeline for generating edge case prompts and testing them against LLMs using Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests  # Retained for general purpose, though not directly for Ollama chat call if ollama library handles it\n",
    "import time\n",
    "import ollama\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import (\n",
    "    display,\n",
    ")  # Added for better display of DataFrames in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Configuration\n",
    "\n",
    "Define settings for our pipeline. We will be using Ollama with the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"artifish/llama3.2-uncensored\"  # Ollama model name\n",
    "OUTPUT_FILE = f\"edge_case_outputs_{MODEL_NAME.replace('/', '_')}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
    "MAX_TOKENS = 4096  # Corresponds to 'num_predict' in Ollama options\n",
    "TEMPERATURE = 0.7  # Corresponds to 'temperature' in Ollama options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Edge Case Prompts\n",
    "\n",
    "We'll load edge case prompts from a JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edge case prompts\n",
    "def load_prompts(file_path):\n",
    "    prompts = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    prompts.append(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing line: {line}. Error: {e}\")\n",
    "    return prompts\n",
    "\n",
    "\n",
    "# Example usage (ensure 'edge_case_prompts.jsonl' exists or adjust path)\n",
    "# prompts = load_prompts('edge_case_prompts.jsonl')\n",
    "# if prompts:\n",
    "# print(f\"Loaded {len(prompts)} edge case prompts\")\n",
    "# else:\n",
    "# print(\"No prompts loaded. Make sure 'edge_case_prompts.jsonl' exists and is not empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Prompts with Ollama\n",
    "\n",
    "We'll use the Ollama client to process each prompt with the `artifish/llama3.2-uncensored` model and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process prompts with Ollama\n",
    "def process_with_ollama(prompt_data):\n",
    "    prompt_text = prompt_data[\"prompt\"]\n",
    "    category = prompt_data.get(\"category\", \"\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Call Ollama API\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "            options={\"temperature\": TEMPERATURE, \"num_predict\": MAX_TOKENS},\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Extract the model's response text\n",
    "        model_response = response[\"message\"][\"content\"]\n",
    "\n",
    "        return {\n",
    "            \"prompt\": prompt_text,\n",
    "            \"category\": category,\n",
    "            \"response\": model_response,\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"time\": elapsed_time,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"success\": True,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        return {\n",
    "            \"prompt\": prompt_text,\n",
    "            \"category\": category,\n",
    "            \"response\": f\"Error: {str(e)}\",\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"time\": elapsed_time,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"success\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Now we'll process all prompts and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all prompts and save results\n",
    "def run_pipeline(prompts_list, output_file_path):\n",
    "    results = []\n",
    "\n",
    "    # Check if prompts_list is None or empty\n",
    "    if not prompts_list:\n",
    "        print(\"No prompts to process. Aborting pipeline run.\")\n",
    "        return results\n",
    "\n",
    "    for prompt_data in tqdm(prompts_list, desc=\"Processing prompts\"):\n",
    "        result = process_with_ollama(prompt_data)\n",
    "        results.append(result)\n",
    "\n",
    "        # Save result incrementally to prevent data loss\n",
    "        with open(output_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "        # Add a small delay to avoid overwhelming the Ollama server if it's local\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a dummy edge_case_prompts.jsonl if it doesn't exist for testing\n",
    "if not os.path.exists(\"edge_case_prompts.jsonl\"):\n",
    "    dummy_prompts = [\n",
    "        {\"prompt\": \"What is the capital of France?\", \"category\": \"general_knowledge\"},\n",
    "        {\n",
    "            \"prompt\": \"Explain quantum computing in simple terms.\",\n",
    "            \"category\": \"technical_explanation\",\n",
    "        },\n",
    "    ]\n",
    "    with open(\"edge_case_prompts.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in dummy_prompts:\n",
    "            f.write(json.dumps(p) + \"\\n\")\n",
    "    print(\"Created dummy 'edge_case_prompts.jsonl' for demonstration.\")\n",
    "\n",
    "prompts = load_prompts(\"edge_case_prompts.jsonl\")\n",
    "if prompts:\n",
    "    print(f\"Loaded {len(prompts)} edge case prompts\")\n",
    "    # Run the pipeline\n",
    "    # Uncomment to execute\n",
    "    # results = run_pipeline(prompts, OUTPUT_FILE)\n",
    "    # print(f\"Completed processing {len(results)} prompts. Results saved to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"No prompts loaded. Pipeline will not run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's analyze the results to identify patterns and issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results from the saved file\n",
    "def analyze_results(file_path):\n",
    "    # Load results\n",
    "    results_list = []  # Renamed to avoid conflict with previous 'results' variable\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Output file {file_path} not found. Run the pipeline first.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    results_list.append(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing line: {line}. Error: {e}\")\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"No results found in the file.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Basic statistics\n",
    "    total = len(results_list)\n",
    "    successful = sum(1 for r in results_list if r.get(\"success\", False))\n",
    "    error_rate = (total - successful) / total if total > 0 else 0\n",
    "    avg_time = sum(r.get(\"time\", 0) for r in results_list) / total if total > 0 else 0\n",
    "\n",
    "    print(f\"Total prompts processed: {total}\")\n",
    "    print(f\"Successful responses: {successful}\")\n",
    "    print(f\"Error rate: {error_rate:.2%}\")\n",
    "    print(f\"Average response time: {avg_time:.2f} seconds\")\n",
    "\n",
    "    # Category analysis if available\n",
    "    if \"category\" in df.columns:\n",
    "        print(\"\\nResults by category:\")\n",
    "        # Ensure 'success' column is boolean or 0/1 for mean calculation\n",
    "        df[\"success_numeric\"] = df[\"success\"].astype(float)\n",
    "        category_stats = df.groupby(\"category\")[\"success_numeric\"].agg([\"count\", \"mean\"])\n",
    "        category_stats.columns = [\"Count\", \"Success Rate\"]\n",
    "        category_stats[\"Success Rate\"] = category_stats[\"Success Rate\"].apply(lambda x: f\"{x:.2%}\")\n",
    "        display(category_stats)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "# Uncomment to execute after running the pipeline\n",
    "# df_results = analyze_results(OUTPUT_FILE)\n",
    "# if not df_results.empty:\n",
    "# print(\"Analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Response Review\n",
    "\n",
    "Let's look at some sample responses to understand the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display sample responses\n",
    "def display_sample_responses(df, n=5):\n",
    "    if df.empty or len(df) == 0:\n",
    "        print(\"No results to display.\")\n",
    "        return\n",
    "\n",
    "    # Sample responses\n",
    "    sample_df = df.sample(min(n, len(df)))\n",
    "\n",
    "    for i, row in enumerate(sample_df.itertuples(), 1):\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        if hasattr(row, \"category\"):\n",
    "            print(f\"Category: {row.category}\")\n",
    "        print(f\"\\nPrompt:\\n{row.prompt}\")\n",
    "        print(f\"\\nResponse:\\n{row.response}\")\n",
    "        if hasattr(row, \"time\"):\n",
    "            print(f\"\\nTime: {row.time:.2f} seconds\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample responses\n",
    "# Uncomment to execute after running analysis and having df_results\n",
    "# if 'df_results' in locals() and not df_results.empty:\n",
    "# display_sample_responses(df_results)\n",
    "# else:\n",
    "# print(\"DataFrame 'df_results' not available or empty. Run analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Edge Cases\n",
    "\n",
    "Based on our findings, we can generate new edge cases that target specific weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate new edge cases\n",
    "def generate_new_edge_cases():\n",
    "    # Define new edge case categories and templates\n",
    "    new_categories = [\n",
    "        {\n",
    "            \"category\": \"instruction_overload\",\n",
    "            \"templates\": [\n",
    "                \"Follow these 20 steps precisely: {steps}\",\n",
    "                \"I need you to perform these actions in sequence: {actions}\",\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"complex_reasoning\",\n",
    "            \"templates\": [\n",
    "                \"Solve this multi-step problem: {problem}\",\n",
    "                \"Analyze this scenario with multiple stakeholders: {scenario}\",\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Generate specific instances\n",
    "    new_prompts_list = []  # Renamed to avoid conflict\n",
    "\n",
    "    # Example for instruction_overload\n",
    "    steps = \"\\n\".join([f\"{i+1}. Perform task {chr(65+i)}\" for i in range(20)])\n",
    "    new_prompts_list.append(\n",
    "        {\n",
    "            \"prompt\": f\"Follow these 20 steps precisely: {steps}\",\n",
    "            \"category\": \"instruction_overload\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Example for complex_reasoning\n",
    "    problem = \"A company needs to allocate resources across 5 projects with different ROIs, time constraints, and team preferences. Project A has a 15% ROI but requires senior staff. Project B has a 12% ROI with no special requirements. Project C has a 20% ROI but a 6-month timeline. Project D has an 18% ROI with high risk. Project E has a 10% ROI but strengthens client relationships. How should they prioritize?\"\n",
    "    new_prompts_list.append(\n",
    "        {\n",
    "            \"prompt\": f\"Solve this multi-step problem: {problem}\",\n",
    "            \"category\": \"complex_reasoning\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save new prompts\n",
    "    output_new_prompts_file = \"new_edge_cases.jsonl\"\n",
    "    with open(output_new_prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for prompt_item in new_prompts_list:\n",
    "            f.write(json.dumps(prompt_item) + \"\\n\")\n",
    "\n",
    "    print(\n",
    "        f\"Generated {len(new_prompts_list)} new edge cases and saved to {output_new_prompts_file}\"\n",
    "    )\n",
    "    return new_prompts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new edge cases\n",
    "# Uncomment to execute\n",
    "# new_edge_cases_list = generate_new_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test New Edge Cases\n",
    "\n",
    "We can now test our newly generated edge cases with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new edge cases\n",
    "def test_new_edge_cases(new_prompts_to_test):\n",
    "    if not new_prompts_to_test:\n",
    "        print(\"No new edge cases to test.\")\n",
    "        return []\n",
    "    output_file_new_results = f\"new_edge_case_results_{MODEL_NAME.replace('/', '_')}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
    "    new_run_results = run_pipeline(new_prompts_to_test, output_file_new_results)\n",
    "    print(\n",
    "        f\"Completed testing {len(new_run_results)} new edge cases. Results saved to {output_file_new_results}\"\n",
    "    )\n",
    "    return new_run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test new edge cases\n",
    "# Uncomment to execute after generating new_edge_cases_list\n",
    "# if 'new_edge_cases_list' in locals() and new_edge_cases_list:\n",
    "# new_results_data = test_new_edge_cases(new_edge_cases_list)\n",
    "# else:\n",
    "# print(\"Variable 'new_edge_cases_list' not available or empty. Generate them first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Summarize findings and recommendations for model improvements based on edge case testing with Ollama and the `artifish/llama3.2-uncensored` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize findings\n",
    "def summarize_findings():\n",
    "    findings_text = f\"\"\"\n",
    "    # Edge Case Testing Findings for {MODEL_NAME}\n",
    "    \n",
    "    ## Strengths\n",
    "    - [Note strengths identified in testing with {MODEL_NAME}]\n",
    "    - [Add more as discovered]\n",
    "    \n",
    "    ## Weaknesses\n",
    "    - [Note weaknesses identified in testing with {MODEL_NAME}]\n",
    "    - [Add more as discovered]\n",
    "    \n",
    "    ## Recommendations\n",
    "    - [List recommendations for model improvements or prompt engineering]\n",
    "    - [Add more as discovered]\n",
    "    \n",
    "    ## Next Steps\n",
    "    - Develop more targeted edge cases for specific weaknesses identified.\n",
    "    - Test with different Ollama parameters (e.g., different temperature, top_k, top_p).\n",
    "    - Compare performance with other models available via Ollama or other platforms.\n",
    "    \"\"\"\n",
    "\n",
    "    print(findings_text)\n",
    "\n",
    "    # Save findings to a file\n",
    "    findings_filename = f\"findings_{MODEL_NAME.replace('/', '_')}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    with open(findings_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(findings_text)\n",
    "    print(f\"Findings saved to {findings_filename}\")\n",
    "    return findings_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize findings\n",
    "# Uncomment to execute\n",
    "# summary_text = summarize_findings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
