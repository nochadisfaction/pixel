{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup for Lightning.ai A100\n",
    "!pip install -q \"torch==2.6.0+cu124\" torchvision torchaudio \\\n",
    "--extra-index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q lightning lightning-ai wandb huggingface_hub protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.strategies import FSDPStrategy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "import wandb\n",
    "from unsloth import FastLanguageModel\n",
    "import datetime\n",
    "\n",
    "\n",
    "def set_seed(seed=3407):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed()\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__} | CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalHealthModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/granite-3.2-2b-instruct\",\n",
    "            max_seq_length=4096,\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.model = FastLanguageModel.get_peft_model(\n",
    "            self.model,\n",
    "            r=32,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=3407,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.model(inputs, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss if hasattr(outputs, \"loss\") else outputs[0]\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss if hasattr(outputs, \"loss\") else outputs[0]\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss if hasattr(outputs, \"loss\") else outputs[0]\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalHealthDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path=\"merged_mental_health_dataset.jsonl\",\n",
    "        batch_size=8,\n",
    "        min_prompt_len=10,\n",
    "        min_response_len=20,\n",
    "        system_prompt=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.min_prompt_len = min_prompt_len\n",
    "        self.min_response_len = min_response_len\n",
    "        self.system_prompt = system_prompt or \"\"\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/granite-3.2-2b-instruct\",\n",
    "            max_seq_length=4096,\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"auto\",\n",
    "        )[1]\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = load_dataset(\"json\", data_files=self.data_path, split=\"train\")\n",
    "\n",
    "        # Data quality filter\n",
    "        def is_valid(example):\n",
    "            return (\n",
    "                example.get(\"prompt\")\n",
    "                and example.get(\"response\")\n",
    "                and len(example[\"prompt\"]) >= self.min_prompt_len\n",
    "                and len(example[\"response\"]) >= self.min_response_len\n",
    "            )\n",
    "\n",
    "        dataset = dataset.filter(is_valid)\n",
    "\n",
    "        # Optional: prepend system prompt\n",
    "        if self.system_prompt:\n",
    "\n",
    "            def add_system_prompt(example):\n",
    "                example[\"prompt\"] = self.system_prompt + example[\"prompt\"]\n",
    "                return example\n",
    "\n",
    "            dataset = dataset.map(add_system_prompt)\n",
    "\n",
    "        # Tokenization\n",
    "        def tokenize(example):\n",
    "            model_inputs = self.tokenizer(\n",
    "                example[\"prompt\"],\n",
    "                truncation=True,\n",
    "                max_length=1024,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            labels = self.tokenizer(\n",
    "                example[\"response\"],\n",
    "                truncation=True,\n",
    "                max_length=1024,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )[\"input_ids\"]\n",
    "            model_inputs[\"labels\"] = labels\n",
    "            return {k: v.squeeze(0) for k, v in model_inputs.items()}\n",
    "\n",
    "        dataset = dataset.map(tokenize, batched=False)\n",
    "\n",
    "        # Train/val/test split\n",
    "        n = len(dataset)\n",
    "        train_size = int(0.85 * n)\n",
    "        val_size = int(0.10 * n)\n",
    "        test_size = n - train_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(3407),\n",
    "        )\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.batch_size, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = FSDPStrategy(\n",
    "    sharding_strategy=\"FULL_SHARD\",\n",
    "    cpu_offload=False,\n",
    "    mixed_precision=True,\n",
    "    activation_checkpointing=True,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints\",\n",
    "    filename=\"granite-mentalhealth-{epoch}-{step}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    every_n_train_steps=50,\n",
    "    save_last=True,\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"kimble\", log_model=True, resume=\"allow\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    strategy=strategy,\n",
    "    max_epochs=4,\n",
    "    precision=\"bf16-mixed\",\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    logger=wandb_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    run_name = f\"mentalhealth-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    wandb.init(\n",
    "        project=\"kimble\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"model\": \"granite-3.2-2b-instruct\",\n",
    "            \"strategy\": \"FSDP\",\n",
    "            \"precision\": \"bfloat16\",\n",
    "            \"batch_size\": 8,\n",
    "            \"lr\": 1e-5,\n",
    "        },\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "    system_prompt = \"You are a helpful mental health assistant. \"\n",
    "    dm = MentalHealthDataModule(system_prompt=system_prompt)\n",
    "    model = MentalHealthModel()\n",
    "    trainer.fit(model, dm)\n",
    "    # Run test evaluation\n",
    "    trainer.test(model, datamodule=dm)\n",
    "    # Save final model\n",
    "    model.model.save_pretrained(\"./final_model\")\n",
    "    model.tokenizer.save_pretrained(\"./final_model\")\n",
    "    wandb.save(\"./final_model/*\")\n",
    "    # GGUF Conversion\n",
    "    !git clone https://github.com/ggerganov/llama.cpp\n",
    "    !python llama.cpp/convert-hf-to-gguf.py ./final_model --outtype f16\n",
    "    !./llama.cpp/quantize granite-3.2-mentalhealth-f16.gguf granite-3.2-mentalhealth-Q4_K_M.gguf q4_k_m\n",
    "    # Log artifacts\n",
    "    wandb.log_artifact(\"granite-3.2-mentalhealth-Q4_K_M.gguf\", type=\"quantized_model\")\n",
    "    # Push to Huggingface Hub\n",
    "    api = HfApi()\n",
    "    api.upload_folder(\n",
    "        folder_path=\"./final_model\",\n",
    "        repo_id=\"oneblackmage/granite-mentalhealth\",\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload trained model and tokenizer\",\n",
    "    )\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7300549,
     "sourceId": 11635594,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
