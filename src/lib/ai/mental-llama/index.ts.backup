
import { getLogger } from '@/lib/utils/logger';
import { envConfig } from '@/config';

import { MentalLLaMAModelProvider } from './models/MentalLLaMAModelProvider';
import { MentalHealthTaskRouter } from './routing/MentalHealthTaskRouter';
import { MentalLLaMAAdapter } from './adapter/MentalLLaMAAdapter';
import { MentalLLaMAPythonBridge } from './bridge/MentalLLaMAPythonBridge';
import { SlackNotificationService } from '@/lib/services/notification/SlackNotificationService';
import type { ICrisisNotificationHandler } from '@/lib/services/notification/NotificationService';

const logger = getLogger('MentalLLaMAFactory');

export { MentalLLaMAAdapter } from './adapter/MentalLLaMAAdapter';
export { MentalLLaMAModelProvider } from './models/MentalLLaMAModelProvider';
export { MentalHealthTaskRouter } from './routing/MentalHealthTaskRouter';
export { MentalLLaMAPythonBridge } from './bridge/MentalLLaMAPythonBridge';
export * from './types'; // Export all types

/**
 * Configuration for the MentalLLaMAFactory.
 */
export interface MentalLLaMAFactoryConfig {
  defaultModelTier?: '7B' | '13B' | string;
  enablePythonBridge?: boolean;
  pythonBridgeScriptPath?: string;
  // Potentially add overrides for keyword rules, LLM category maps, etc.
}

export class MentalLLaMAFactory {
  /**
   * Creates and configures the MentalLLaMA components.
   * This factory initializes the model provider, task router, crisis notification service (if configured),
   * Python bridge (if enabled), and finally the adapter that ties them all together.
   */
  static async create(config: MentalLLaMAFactoryConfig = {}): Promise<{
    adapter: MentalLLaMAAdapter;
    modelProvider: MentalLLaMAModelProvider;
    taskRouter: MentalHealthTaskRouter;
    pythonBridge?: MentalLLaMAPythonBridge;
    crisisNotifier?: ICrisisNotificationHandler;
  }> {
    logger.info('Creating MentalLLaMA components via factory...', { config });

    const modelTier = config.defaultModelTier || envConfig.mentalLLaMA.defaultModelTier() || '7B';
    const modelProvider = new MentalLLaMAModelProvider(modelTier);

    // The LLMInvoker for the router will be the chat method of the model provider.
    // TODO: Initialize task router and crisis notifier properly
    /*
    const llmInvokerForRouter = modelProvider.chat.bind(modelProvider);
    const taskRouter = new MentalHealthTaskRouter(llmInvokerForRouter);

    let crisisNotifier: ICrisisNotificationHandler | undefined = undefined;
    const slackWebhookUrl = envConfig.notifications.slackWebhookUrl();
    */
    if (slackWebhookUrl) {
      try {
        crisisNotifier = new SlackNotificationService(slackWebhookUrl);
        logger.info('SlackNotificationService initialized for MentalLLaMAAdapter.');
      } catch (error) {
        logger.error('Failed to initialize SlackNotificationService for MentalLLaMAAdapter:', error);
        // Continue without crisis notifications if Slack setup fails
      }
    } else {
      logger.warn('Slack webhook URL not configured. MentalLLaMAAdapter will operate without Slack crisis notifications.');
    }

    let pythonBridge: MentalLLaMAPythonBridge | undefined = undefined;
    if (config.enablePythonBridge || envConfig.mentalLLaMA.enablePythonBridge()) {
      try {
        pythonBridge = new MentalLLaMAPythonBridge(config.pythonBridgeScriptPath);
        // Initialize the bridge. In a real scenario, you might want to ensure this completes
        // successfully before proceeding or handle failures gracefully.
        await pythonBridge.initialize();
        if (pythonBridge.isReady()) {
            logger.info('MentalLLaMAPythonBridge initialized and ready.');
        } else {
            logger.warn('MentalLLaMAPythonBridge initialization failed or did not complete. Features requiring it may not work.');
            // Optionally set pythonBridge back to undefined if it's not usable
        }      } catch (error) {
        logger.error('Failed to initialize MentalLLaMAPythonBridge:', error);
      }

      // TODO: Fix OpenAI provider initialization
      // Initialize OpenAI provider
      /* try {
        modelProvider = new OpenAIModelProvider()
        await modelProvider.initialize(providerOptions)
        logger.info('OpenAIModelProvider initialized successfully.')
      } catch (error) {
        logger.error(
          'Failed to initialize OpenAIModelProvider. LLM functionalities will be limited.',
          {
            error: error instanceof Error ? error.message : String(error),
          },
        )
      } */
  } else {
    logger.warn(
      'OpenAI API key not configured. LLM functionalities will be limited to stubs or keyword-based logic.',
    )
  }

  // 2. Initialize Crisis Notifier (SlackNotificationService)
  let crisisNotifier: ICrisisNotificationHandler | undefined = undefined
  const slackWebhookUrl = config.notifications.slackWebhookUrl()
  if (slackWebhookUrl) {
    try {
      crisisNotifier = new SlackNotificationService(slackWebhookUrl)
      logger.info('SlackNotificationService initialized successfully.')
    } catch (error) {
      logger.error(
        'Failed to initialize SlackNotificationService. Crisis notifications will be disabled.',
        {
          error: error instanceof Error ? error.message : String(error),
          isWebhookUrlPresent: !!slackWebhookUrl,
        },
      )
    }
  } else {
    logger.warn(
      'Slack webhook URL not configured. Crisis notifications via Slack will be disabled.',
    )
  }

  // TODO: Fix LLMInvoker and TaskRouter initialization
  // 3. Create Production-Grade LLMInvoker for TaskRouter
  /* const llmInvokerForRouter: LLMInvoker = await createProductionLLMInvoker(
    modelProvider,
    DEFAULT_PRODUCTION_CONFIG.router,
  )
  logger.info(
    `LLMInvoker for TaskRouter created (ModelProvider available: ${!!modelProvider}).`,
  )

  // 4. Initialize MentalHealthTaskRouter with production configuration
  const routerOptions = {
    defaultTargetAnalyzer: 'general_mental_health',
    defaultConfidence: 0.1,
    maxRetries: 3, // Production: more retries for reliability
    llmTimeoutMs: 45000, // Production: longer timeout for complex requests
    enableFallbackClassification: true, // Always enable fallback in production
    getDefaultConfidence: (context?: RoutingContext) => {
      // Context-dependent confidence logic for production
      if (context?.sessionType === 'crisis_intervention_follow_up') {
        return 0.4
      }
      if (context?.explicitTaskHint === 'safety_screen') {
        return 0.6
      }
      if (context?.previousConversationState?.riskLevel === 'high') {
        return 0.5
      }
      return 0.2
    },
  } */

  // TODO: Implement actual router initialization
  const taskRouter = undefined; // Temporary stub
  const crisisNotifier = undefined; // Temporary stub

  // 5. Initialize PythonBridge (STUBBED)
  const pythonBridgeStub = undefined
  logger.info('PythonBridge (stubbed as undefined) created.')

  // TODO: Fix return statement for development build
  const stubAdapter = {
    analyzeText: async () => ({ category: 'general', confidence: 0.5 }),
    findSuggestedResources: async () => [],
    performSafetyScreening: async () => ({ riskLevel: 'low', reasoning: 'stub' }),
    classifyConversation: async () => ({ category: 'general', confidence: 0.5 }),
    extractEmotions: async () => ({ emotions: [], confidence: 0.5 }),
    generateMCQQuestions: async () => ({ questions: [], metadata: {} }),
    createContextualSummary: async () => ({ summary: 'stub', keyPoints: [] }),
    detectCognitiveDistortions: async () => ({ distortions: [], suggestions: [] }),
    findMissingTopics: async () => ({ gaps: [], suggestions: [] }),
    generateEmpathicResponse: async () => ({ response: 'stub', tone: 'neutral' }),
    generateConversationMetadata: async () => ({ metadata: {} }),
    analyzeText_full: async () => ({ category: 'general', confidence: 0.5 }),
    analyzeText_deprecated: async () => ({ category: 'general', confidence: 0.5 }),
  };

  return {
    adapter: stubAdapter as any,
    modelProvider,
    taskRouter: taskRouter as any,
    pythonBridge: pythonBridgeStub,
    crisisNotifier: crisisNotifier as any,
  }

  /* // 6. Initialize MentalLLaMAAdapter
  const adapterOptions: MentalLLaMAAdapterOptions = {
    taskRouter: taskRouter,
  }

  if (modelProvider) {
    // Create an adapter to bridge the interface mismatch
    const modelProviderAdapter: MentalLLaMAIModelProvider = {
      async invoke(
        messages: Array<{
          role: 'system' | 'user' | 'assistant'
          content: string
        }>,
        options?: LLMInvocationOptions,
      ): Promise<LLMResponse> {
        // Convert LLMInvocationOptions to ChatCompletionOptions
        const chatOptions = options
          ? {
              ...(options.model && { model: options.model }),
              ...(options.temperature !== undefined && {
                temperature: options.temperature,
              }),
              ...(options.max_tokens !== undefined && {
                max_tokens: options.max_tokens,
              }),
              ...(options.top_p !== undefined && { top_p: options.top_p }),
              ...(options.frequency_penalty !== undefined && {
                frequency_penalty: options.frequency_penalty,
              }),
              ...(options.presence_penalty !== undefined && {
                presence_penalty: options.presence_penalty,
              }),
              ...(options.stop && { stop: options.stop }),
            }
          : undefined

        const response = await modelProvider.chatCompletion(
          messages,
          chatOptions,
        )
        if (response.error) {
          throw new Error(response.error.message)
        }

        const result: LLMResponse = {
          content: response.choices[0]?.message?.content || '',
          model: response.model,
          metadata: {
            id: response.id,
            object: response.object,
            created: response.created,
          },
        }

        if (response.choices[0]?.finish_reason) {
          result.finishReason = response.choices[0].finish_reason as
            | 'stop'
            | 'length'
            | 'content_filter'
            | 'function_call'
        }

        if (response.usage) {
          result.tokenUsage = {
            promptTokens: response.usage.prompt_tokens,
            completionTokens: response.usage.completion_tokens,
            totalTokens: response.usage.total_tokens,
          }
        }

        return result
      },
      getModelInfo() {
        return {
          name: modelProvider.getProviderName(),
          version: 'latest',
          capabilities: ['chat_completion', 'text_generation'],
        }
      },
      async isAvailable() {
        try {
          // Test availability by trying a simple completion
          await modelProvider.chatCompletion(
            [{ role: 'user', content: 'test' }],
            { max_tokens: 1 },
          )
          return true
        } catch {
          return false
        }
      },
    }
    adapterOptions.modelProvider = modelProviderAdapter
  }

  if (crisisNotifier) {
    adapterOptions.crisisNotifier = crisisNotifier
  }

  // Don't include pythonBridge if it's undefined
  // if (pythonBridgeStub) {
  //   adapterOptions.pythonBridge = pythonBridgeStub;
  // }

  const adapter = new MentalLLaMAAdapter(adapterOptions)
  logger.info('MentalLLaMAAdapter initialized successfully by factory.')

  return {
    adapter,
    modelProvider,
    pythonBridge: pythonBridgeStub,
    taskRouter,
    crisisNotifier,
  }
  */ // End of commented code
}
